\documentclass[oneside]{book}
\usepackage[utf8]{inputenc}
\usepackage{xeCJK}
\usepackage{bm}
\usepackage{geometry}
\usepackage{fontspec} 
\usepackage[normalem]{ulem}
\setCJKmainfont{PingFangSC-Regular}
\setCJKmonofont{PingFangSC-Regular}
\font\titlteFont=NovaMono at 40pt
\font\authorFont=NovaMono at 24pt
\font\NovaMonoFont=NovaMono
\title{\titlteFont Machine Learning Notes}
\author{\authorFont{Jiang Tao}\\ \NovaMonoFont{vix2018@gmail.com}}
\date{\NovaMonoFont 2019}
\begin{document}
	\begin{titlepage}
		\maketitle
	\end{titlepage}
	\tableofcontents
	\setcounter{chapter}{0}
	\renewcommand{\thechapter}{\roman{chapter}}
	\chapter{主要内容}
			\begin{enumerate} 
				\item 特征处理: 对数据进行降维(Dimensionality Reduction) 和特征缩放(Feature Scaling) 等.
				\item 模型: 线性模型, 神经网络, SVM 等.
				\item 定义损失函数: 根据不同的模型, 找出适合它的损失函数.
				\item 最优化: 梯度下降等.
				\item 模型评估: 对模型进行评估与预防过拟合.
				\item 模型分析: 使用一些手段对某个模型(DNN)进行解剖分析.(待定)
			\end{enumerate}
	\chapter{所需知识}
		\section{线性代数}
			\subsection{矩阵分解}
				\subsubsection{Eigen-Value Decomposition}
					矩阵分解为由其特征值和特征向量表示的矩阵之积.即:
					$$\boldsymbol{A} = \lambda \boldsymbol{u}$$
				\subsubsection{Singular-Value Decomposition}
					SVD是线性代数中一种重要的矩阵分解, 其结果可以直接用于PCA.				
		\section{概率论}
			\subsection{多维正太分布(Multivariate Gaussian Distribution)}	
				若$N$维随机变量$\boldsymbol{X}= [X_{1}, X_{2} ... X_{n}]^{T}$服从多元正太分布, 且其协方差矩
				阵非奇异, 则PDF可表示为:
			\subsection{Expectation-Maximization algorithm}
				最大期望算法, 可用于计算PPCA.
				
				$$f_{\boldsymbol{X}}(x_{1}, ... , x_{n}) = \frac{1}{\sqrt{(2\pi)^{n}|\boldsymbol{\Sigma}|}}\exp{\left(-\frac{1}{2}(\boldsymbol{X}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{X}-\boldsymbol{\mu})\right)}$$
				其中$\boldsymbol{\Sigma}$是协方差矩阵, $\boldsymbol{\mu}$是均值.
		
		\section{信息论}
	\setcounter{chapter}{0}
	\renewcommand{\thechapter}{\arabic{chapter}}
	\chapter{特征处理}
		\section{特征缩放(Feature Scaling)}
			\subsection{作用}
				由于各个特征的范围不一, 导致在某些模型下训练效果极差. 使用特征缩放有助于模型的训练.
			\subsection{方法}
				\subsubsection{Min-Max Normalization}
					$$x'_i = \frac{x_i - \min{(X)}}{\max{(X)} - \min{(X)}},~x_i\in X$$
				\subsubsection{Mean Normalization}
					$$x'_i = \frac{x_i - average(X)}{\max{(X)} - \min{(X)}},~x_i\in X$$
				\subsubsection{Standardization}
					$$x'_i = \frac{x_i - \mu}{\sigma},~x_{i}\in X$$
				\subsubsection{Scaling to Unit Length}
				 	$$x'_i = \frac{x_{i}}{||x_{i}||}, ~x_i \in X$$
		\section{降维(Dimensionality Reduction)}
			\subsection{特征选择(Feature Selection)}
				\subsubsection{Pooling}
					作用: 应对图片的放缩, 旋转, 等线性变换.
					方法:
					\begin{itemize}
						\item Max Pooling
						\item Average Pooling
						\item Weighted average based on the distance from the central pixel
					\end{itemize}
				\subsubsection{手动特征选择}
					根据实际问题, 经验自行判断某一特征的必要性.
			\subsection{特征投影(Feature Projection)}
				\subsubsection{主成成分分析(Principal Component Analysis)}
					\emph{PCA}\\
					通过对协方差矩阵进行特征分解, 以得出数据的主成分(即特征向量) 与它们的权值(即特征值).
					在原数据中除掉最小的特征值所对应的成分, 那么所得的低维度数据必定失去讯息最少.\\
					一个正交化线性变换，把数据变换到一个新的坐标系统中, 使得这一数据的任何投影的第一大方差
					在第一个坐标(称为第一主成分) 上, 第二大方差在第二个坐标(第二主成分) 上, 依次类推.\\ 
					投影至低维超平面上的点会很分开(方差最大).\\ \\
					\emph{PPCA(Probability Principal Component Analysis)}\\
					PCA从概率的方向利用EM进行推导, 就是PCA. \\ \\
					\emph{核化线性降维 Kernel PCA}\\
					一种非线性的PCA方法(类似于将高维空间中的点按照一个形状拉直?)\\ \\
					\emph{Graph-based kernel PCA}\\
				\subsubsection{NMF}
				\subsubsection{LDA}
				\subsubsection{GDA}
				\subsubsection{PLSA}
				\subsection{Auto-Encoder}
				\subsection{流形学习 Manifold Learning}
					\subsubsection{等度量映射 Isometric Mapping}
					\subsubsection{局部线性嵌入 Locally Linear Embedding}
			\subsection{多维缩放 Multiple Dimensional Scaling}
			
		
	\chapter{模型}
		\section{Multilayer Perceptron}
	\chapter{最优化}
		\section{SGD}
	\chapter{模型评估}
\end{document}