\documentclass[oneside]{book}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks,linkcolor=black,anchorcolor=black,citecolor=black]{hyperref}
\usepackage{xeCJK}
\usepackage{bm}
\usepackage{geometry}
\usepackage{fontspec} 
\usepackage[normalem]{ulem}
\setCJKmainfont{PingFangSC-Regular}
\setCJKmonofont{PingFangSC-Regular}
\font\titlteFont=NovaMono at 40pt
\font\authorFont=NovaMono at 24pt
\font\NovaMonoFont=NovaMono
\title{\titlteFont Machine Learning Notes}
\author{\authorFont{Jiang Tao}\\ \NovaMonoFont{vix2018@gmail.com}}
\date{\NovaMonoFont 2019}
\begin{document}
	\begin{titlepage}
		\maketitle
	\end{titlepage}
	\tableofcontents
	\setcounter{chapter}{0}
	\renewcommand{\thechapter}{\Roman{chapter}}
	\chapter{主要内容}
			\begin{enumerate} 
				\item 特征处理: 对数据进行降维(Dimensionality Reduction) 和特征缩放(Feature Scaling) 等.
				\item 模型: 线性模型, 神经网络, SVM 等.
				\item 定义损失函数: 根据不同的模型, 找出适合它的损失函数.
				\item 优化器: 梯度下降等.
				\item 模型评估: 对模型进行评估与预防过拟合.
				\item 模型分析: 使用一些手段对某个模型(DNN)进行解剖分析.(待定)
			\end{enumerate}
	\chapter{所需知识}
		\section{线性代数}
			\subsection{矩阵分解}
				\subsubsection{Eigen-Value Decomposition}
					矩阵分解为由其特征值和特征向量表示的矩阵之积.即:
					$$\boldsymbol{A} = \lambda \boldsymbol{u}$$
				\subsubsection{Singular-Value Decomposition}
					SVD是线性代数中一种重要的矩阵分解, 其结果可以直接用于PCA.	
				\subsubsection{Non-Negative Matrix Factorization}	
		\section{概率论}
			\subsection{多维正太分布(Multivariate Gaussian Distribution)}	
				若$N$维随机变量$\boldsymbol{X}= [X_{1}, X_{2} ... X_{n}]^{T}$服从多元正太分布, 且其协方差矩
				阵非奇异, 则PDF可表示为:				
				$$f_{\boldsymbol{X}}(x_{1}, ... , x_{n}) = \frac{1}{\sqrt{(2\pi)^{n}|\boldsymbol{\Sigma}|}}\exp{\left(-\frac{1}{2}(\boldsymbol{X}-\boldsymbol{\mu})^{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{X}-\boldsymbol{\mu})\right)}$$
				其中$\boldsymbol{\Sigma}$是协方差矩阵, $\boldsymbol{\mu}$是均值.
				\subsection{Expectation-Maximization algorithm}
				最大期望(EM)算法, 可用于计算PPCA.
		\section{信息论}
			\subsection{信息熵}
			\subsection{交叉熵}
			\subsection{相对熵}
	\setcounter{chapter}{0}
	\renewcommand{\thechapter}{\arabic{chapter}}
	\chapter{特征处理}
		\section{特征缩放(Feature Scaling)}
			\subsection{作用}
				由于各个特征的范围不一, 导致在某些模型下训练效果极差. 使用特征缩放有助于模型的训练.
			\subsection{方法}
				\subsubsection{Min-Max Normalization}
					$$x'_i = \frac{x_i - \min{(X)}}{\max{(X)} - \min{(X)}},~x_i\in X$$
				\subsubsection{Mean Normalization}
					$$x'_i = \frac{x_i - average(X)}{\max{(X)} - \min{(X)}},~x_i\in X$$
				\subsubsection{Standardization}
					$$x'_i = \frac{x_i - \mu}{\sigma},~x_{i}\in X$$
				\subsubsection{Scaling to Unit Length}
					 $$x'_i = \frac{x_{i}}{||x_{i}||}, ~x_i \in X$$
			\section{特征选择(Feature Selection)}
				\subsection{手动特征选择}
					根据实际问题, 经验自行判断某一特征的必要性.
				\subsection{Pooling}
					作用: 应对图片的放缩, 旋转, 等线性变换.\\
					方法: \dots
					\begin{itemize}
						\item Max Pooling
						\item Global Max Pooling
						\item Average Pooling
						\item Global Average Pooling
					\end{itemize}

				\subsection{过滤式选择}

				\subsection{包裹式选择}

				\subsection{稀疏表示 (Sparse Representation)}

				\subsection{字典学习 (Dictionary Learning)}

				\subsection{压缩感知 (Compressed Sensing)}

		\section{降维(Dimensionality Reduction)}
				\subsection{主成成分分析(Principal Component Analysis)}
					利用数学方法降维可以分为线性降维(Linear Dimensionality Reduction)与非线性降维(Nonlinear Dimensionality Reduction), 不仅仅是PCA的变种会有非线性降维, 其他的非PCA方法也有非线性的方法. 今后若要寻找其他数学降维方法, 可根据这两个方向寻找.
					\subsubsection{PCA \NovaMonoFont{[Linear]}}
					PCA的思想: 通过对协方差矩阵进行特征分解, 以得出数据的主成分(即特征向量) 与它们的权值(即特征值).
					在原数据中除掉最小的特征值所对应的成分, 那么所得的低维度数据必定失去讯息最少.
					一个正交化线性变换，把数据变换到一个新的坐标系统中, 使得这一数据的任何投影的第一大方差
					在第一个坐标(称为第一主成分) 上, 第二大方差在第二个坐标(第二主成分) 上, 依次类推.
					投影至低维超平面上的点会很分开(方差最大).\\ \\
					PCA的数学推导:\\
					\subsubsection{PPCA(Probability Principal Component Analysis) \NovaMonoFont{[Linear]}}
					PCA从概率的方向利用EM进行推导, 就是PCA. 
					\subsubsection{核化线性降维 Kernel PCA \NovaMonoFont{[Nonlinear]}}
					一种非线性的PCA方法(类似于将高维空间中的点按照一个形状拉直?)
					\subsubsection{Graph-based kernel PCA \NovaMonoFont{[Nonlinear]}}
					
				
				\subsection{自动编码机(Auto-Encoder)}
				\subsection{流形学习 (Manifold Learning) }
					\subsubsection{等度量映射 (Isometric Mapping)  \NovaMonoFont{[Nonlinear]}}
					\subsubsection{局部线性嵌入 (Locally Linear Embedding)  \NovaMonoFont{[Linear]}}
			\subsection{多维缩放 (Multiple Dimensional Scaling)}
			\subsection{度量学习 (Metric Learning)}
			
			
		
	\chapter{模型}
		%对于模型的描述主要分为: 模型的思想、模型的使用场景、模型的算法实现、模型的数学解释、模型的优缺点
		\section{Linear Model}
		\section{Decision Tree}
		\section{Neural Network}
			\subsection{Multilayer Perceptron}
		\section{Support Vector Machine}
		\section{K-neighbors}
		\section{Bayesian}
		\section{Gaussian Discriminant Analysis}
		\section{(Probabilistic) Graphical Models}
		\section{Latent Dirichlet Allocation \NovaMonoFont{[Topic Model]}}
		\section{Latent Semantic Analysis \NovaMonoFont{[Topic Model]}}
		\section{Probability Latent Semantic Analysis \NovaMonoFont{[Topic Model]}}
		
	\chapter{损失函数}
		%思想 or 数学论证
		\section{Regression Losses}
			\subsection{Mean Square Error}
			\subsection{Mean Absolute Error}
			\subsection{Mean Bias Error}
			\subsection{Logarithm of the hyperbolic cosin}
			\subsection{Huber Loss}
		\section{Classification Losses}
			\subsection{Hinge loss}
				\subsubsection{Hinge}
				\subsubsection{Squared Hinge}
				\subsubsection{Categorical Hinge}
			\subsection{Logistic loss}
			\subsection{Exponential loss}
			\subsection{Cross Entropy Loss}
				\subsubsection{Categorical Crossentropy}
				\subsubsection{Sparse Categorical Crossentropy}
				\subsubsection{Binary Crossentropy}
			\subsection{Multi class SVM Loss}
		\section{Relative Entropy}
		\section{Regularization}

	\chapter{激活函数}
		\section{Sigmoid}
		\section{Hard Sigmoid}
		\section{Softmax}
		\section{Exponential Linear Unit}
		\section{Scaled Exponential Linear Unit}
		\section{Rectified Linear Unit}
		\section{Identity Activation Function}
		\section{Softplus}
		\section{Softsign}
		\section{Hyperbolic Tangent}
		\section{Exponential (base e)}

	\chapter{噪音}
		\section{Gaussian Noise}
		\section{Gaussian Dropout}
		\section{Alpha Dropout}

	\chapter{初始化}
		\section{Normal distribution}
		\section{Uniform distribution}
		\section{Truncated Normal distribution}
		\section{Variance Scaling}
		\section{Random Orthogonal Matrix}

	\chapter{优化器}
		\section{Mini-batch	Gradient descent}
		\section{Stochastic Gradient Descent}
			\subsection{Implicit-updates SGD}
			\subsection{Averaged SGD}
			\subsection{Kalman-Based SGD}
			SGD的变种可向两个方向进行发展:
			\begin{itemize}
				\item 动量模拟: Momentum.
				\item 自适应 Learning rate: Adaptive.
			\end{itemize}
			\subsection{Momentum}
			\subsection{Nesterov Accelerated Gradient \NovaMonoFont{[Momentum]}}
				Standard Momentum: 先计算当前梯度与动量的方向, 然后进行跳跃.
				Nesterov Momentum: 先根据之前的动量方向进行跳跃, 然后再根据跳跃终点的梯度进行修正.
				
			\subsection{Adagrad \NovaMonoFont{[Adaptive]}}
			\subsection{Root Mean Square Propagation(RMSProp) \NovaMonoFont{[Adaptive]}}
			\subsection{AdaDelta \NovaMonoFont{[Adaptive]}}
			\subsection{Adam \NovaMonoFont{[Momentum \& Adaptive]}}
			\subsection{Adam Max \NovaMonoFont{[Momentum \& Adaptive]}}
			
		
		
	\chapter{模型评估}
		\section{经验误差与过拟合}
			\subsection{检测}
			\subsection{预防}
				\subsubsection{Noise}
				\subsubsection{Normalization}
				\subsubsection{Regularization}
		\section{集合采样方法}
			\subsection{留出 (Hold-out)}
			\subsection{交叉检验 (Cross Validation)}
			\subsection{Bootstrapping}
		\section{评估模型性能}
			\subsection{均方误差 (Mean Squared Error)}
				用于回归问题.
			\subsection{错误率 \& 精度}
				统计错误或正确的个数, 多用于分类问题.
			\subsection{查准率}
			\subsection{查全率}
			\subsection{P-R曲线 \& Break-Even Point}
			\subsection{受试者工作特征 (Receiver Operating Characteristic)}
				%AUC & ROC
			\subsection{代价敏感错误率}
			\subsection{假设检验 [统计假设检验]}
			\subsection{交叉验证t检验 [统计假设检验]}
			\subsection{McNemar 检验 [统计假设检验]}
		\section{偏差与方差}
			%偏差 bias: 准星与靶心的距离
			%方差 variance: 子弹的散开程度
\end{document}